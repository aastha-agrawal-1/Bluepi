{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T09:44:12.972996Z",
     "start_time": "2020-04-16T09:44:12.969414Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T09:44:17.593030Z",
     "start_time": "2020-04-16T09:44:13.633706Z"
    }
   },
   "outputs": [],
   "source": [
    "#starting spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName('Second use case in pyspark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T09:44:17.634969Z",
     "start_time": "2020-04-16T09:44:17.598952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16-04-2020\n",
      "07-04-2020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\none_day_before_yesterday = yesterday - datetime.timedelta(days=1)\\none_day_before_yesterday_str=one_day_before_yesterday.strftime(\"%d-%m-%Y\")\\nprint(one_day_before_yesterday_str)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding today's and yesterday's date\n",
    "today=datetime.date.today()\n",
    "today_str=today.strftime(\"%d-%m-%Y\")\n",
    "print(today_str)\n",
    "yesterday = today - datetime.timedelta(days=9)\n",
    "yesterday_str=yesterday.strftime(\"%d-%m-%Y\")\n",
    "print(yesterday_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T09:44:21.771248Z",
     "start_time": "2020-04-16T09:44:21.764219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bluepi/Downloads/result_2/tmpsmrq0ole'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making temporary folder\n",
    "f  =  tempfile.TemporaryDirectory(dir  =  \"/home/bluepi/Downloads/result_2\")\n",
    "f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T09:45:06.067653Z",
     "start_time": "2020-04-16T09:45:06.056989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-dc9c1bff-067d-49d9-9304-9ce97073fab1-c000.csv\n"
     ]
    }
   ],
   "source": [
    "#file which have to be fetched\n",
    "t=os.listdir('/home/bluepi/Downloads/result_2/latest_product/')\n",
    "for file in t:\n",
    "        if file.endswith(\".csv\"):\n",
    "            name=file\n",
    "            \n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:02:47.330450Z",
     "start_time": "2020-04-15T13:02:47.322994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bluepi/Downloads/result_2/tmpl_1er0ju/part-00000-d42eda1d-2286-47a9-b456-b776c4551e34-c000.csv\n"
     ]
    }
   ],
   "source": [
    "#copying file which have to be read\n",
    "path=shutil.copy2(os.path.join('/home/bluepi/Downloads/result_2/latest_product/',name),f.name)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:02:57.941983Z",
     "start_time": "2020-04-15T13:02:57.770220Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading the main table\n",
    "try:\n",
    "    main_table=spark.read.csv(path,header=True)\n",
    "except:\n",
    "    print(\"File does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:00.239582Z",
     "start_time": "2020-04-15T13:03:00.115718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+--------------------+\n",
      "|p_id|     p_name|price|      Date_timestamp|\n",
      "+----+-----------+-----+--------------------+\n",
      "|   1|       Rank|  956|2020-01-20T14:39:...|\n",
      "|   2|Mat Lam Tam|  556|2020-02-05T02:44:...|\n",
      "|   3|   Greenlam|   17|2020-02-07T03:31:...|\n",
      "|   4|         It|   55|2020-01-25T17:10:...|\n",
      "|   5|  Holdlamis|  186|2020-02-21T04:16:...|\n",
      "|   6|     Latlux|  181|2020-01-13T20:25:...|\n",
      "|   7|   Greenlam|  294|2020-02-28T09:00:...|\n",
      "|   8|   Alphazap|  739|2020-01-17T04:09:...|\n",
      "|   9|   Alphazap|  423|2020-02-26T14:30:...|\n",
      "|  10|   Home Ing|  681|2020-04-04T21:40:...|\n",
      "+----+-----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total rows in main_table= 104\n"
     ]
    }
   ],
   "source": [
    "#showcasing the contents of table and counting the number of records\n",
    "main_table.show(10)\n",
    "print(\"Total rows in main_table=\",main_table.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:05.576925Z",
     "start_time": "2020-04-15T13:03:05.549300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_id: string (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- Date_timestamp: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- p_id: integer (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- Date_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the schema\n",
    "main_table.printSchema()\n",
    "\n",
    "#updating the datatpe of columns\n",
    "mt1 = main_table.withColumn(\"p_id\",main_table['p_id'].cast(\"int\"))\\\n",
    "                .withColumn(\"price\",main_table['price'].cast(\"int\"))\\\n",
    "                .withColumn(\"Date_timestamp\", main_table['Date_timestamp'].cast(\"timestamp\"))\n",
    "\n",
    "#checking the schema\n",
    "mt1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:06.613211Z",
     "start_time": "2020-04-15T13:03:06.610776Z"
    }
   },
   "outputs": [],
   "source": [
    "#making path as a variable\n",
    "folder_path='/home/bluepi/Downloads/use_case_2/'\n",
    "path=folder_path+yesterday_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:07.675255Z",
     "start_time": "2020-04-15T13:03:07.532791Z"
    }
   },
   "outputs": [],
   "source": [
    "#reading the files from the folder\n",
    "\n",
    "try:\n",
    "    changes_to_be_done=spark.read.csv(path,header=True)\n",
    "except:\n",
    "    print(\"Folder does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:08.891059Z",
     "start_time": "2020-04-15T13:03:08.409430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+-------------------+-----------+\n",
      "|p_id|     p_name|price|     date_timestamp|record_type|\n",
      "+----+-----------+-----+-------------------+-----------+\n",
      "|  13|    Konklux|  866|2020-04-05 04:00:18|          U|\n",
      "| 198|    Cookley|  304|2020-04-05 03:35:35|          U|\n",
      "|  55|      Opela|   50|2020-04-05 03:15:21|          U|\n",
      "|  14|Stringtough|  199|2020-04-05 03:54:35|          U|\n",
      "|  87| Trippledex|   48|2020-04-05 03:43:00|          U|\n",
      "| 175|    Fixflex|  498|2020-04-05 03:25:24|          I|\n",
      "| 168|     Zathin|  259|2020-04-05 03:47:55|          I|\n",
      "|  47|     Sonair|  547|2020-04-05 03:12:40|          D|\n",
      "|  50|    Matsoft|  963|2020-04-05 03:20:45|          D|\n",
      "|  18|  Cardguard| 1066|2020-04-05 21:25:10|          U|\n",
      "+----+-----------+-----+-------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|record_type|count|\n",
      "+-----------+-----+\n",
      "|          U|   17|\n",
      "|          D|    8|\n",
      "|          I|    7|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showcasing the contents of table\n",
    "changes_to_be_done.show(10)\n",
    "changes_to_be_done.groupBy('record_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:09.517563Z",
     "start_time": "2020-04-15T13:03:09.498740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_id: string (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- date_timestamp: string (nullable = true)\n",
      " |-- record_type: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- p_id: integer (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- Date_timestamp: timestamp (nullable = true)\n",
      " |-- record_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the schema\n",
    "changes_to_be_done.printSchema()\n",
    "\n",
    "#updating the datatpe\n",
    "ctbd1 = changes_to_be_done.withColumn(\"p_id\",changes_to_be_done['p_id'].cast(\"int\"))\\\n",
    "                          .withColumn(\"price\",changes_to_be_done['price'].cast(\"int\"))\\\n",
    "                          .withColumn(\"Date_timestamp\", changes_to_be_done['Date_timestamp'].cast(\"timestamp\"))\n",
    "\n",
    "#checking the schema\n",
    "ctbd1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:10.491491Z",
     "start_time": "2020-04-15T13:03:10.468990Z"
    }
   },
   "outputs": [],
   "source": [
    "#registering the dataframe in tables\n",
    "\n",
    "mt1.registerTempTable('Product_table')\n",
    "ctbd1.registerTempTable('Changes_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:17.157067Z",
     "start_time": "2020-04-15T13:03:14.962954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inserted= 7\n",
      "Total rows in main table after inserting= 111\n",
      "New inserted id's= [102, 148, 158, 168, 175, 185, 198]\n"
     ]
    }
   ],
   "source": [
    "#for inserting the data\n",
    "\n",
    "#to retrieve the latest data from the data to be inserted\n",
    "table=spark.sql(\"\"\" select c1.p_id,c1.p_name,c1.price,c1.Date_timestamp\n",
    "                    from Changes_table as c1\n",
    "                    inner join\n",
    "                    (\n",
    "                    select p_id, max(Date_timestamp) as Datetime\n",
    "                    from Changes_table\n",
    "                    where record_type='I'\n",
    "                    group by p_id\n",
    "                    ) as c2\n",
    "                    on c1.p_id=c2.p_id and c1.Date_timestamp=c2.Datetime\n",
    "                    order by c1.p_id\"\"\")\n",
    "\n",
    "#converting dataframe into table\n",
    "table.registerTempTable('Table')\n",
    "\n",
    "\n",
    "#to insert data\n",
    "table1=spark.sql(\"\"\" select * \n",
    "                    from Product_table\n",
    "                    union\n",
    "                    select * \n",
    "                    from Table\"\"\")\n",
    "\n",
    "\n",
    "#converting dataframe with inserted values into table\n",
    "table1.registerTempTable('Data_after_insertion')\n",
    "\n",
    "\n",
    "#printing the details\n",
    "print(\"Total inserted=\",table.count())\n",
    "print(\"Total rows in main table after inserting=\",table1.count())\n",
    "print(\"New inserted id's=\",[int(row.p_id) for row in table.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:23.386227Z",
     "start_time": "2020-04-15T13:03:18.847186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total updated= 16\n",
      "Total rows in main table after updating= 111\n",
      "New updated id's= [2, 12, 13, 14, 18, 32, 37, 55, 76, 79, 87, 90, 140, 153, 174, 198]\n"
     ]
    }
   ],
   "source": [
    "#for updating the data\n",
    "\n",
    "#to retrieve the latest data from the data to be updated\n",
    "table=spark.sql(\"\"\" select c1.p_id,c1.p_name,c1.price,c1.Date_timestamp\n",
    "                    from Changes_table as c1\n",
    "                    inner join\n",
    "                    (\n",
    "                    select p_id, max(Date_timestamp) as Datetime\n",
    "                    from Changes_table\n",
    "                    where record_type='U'\n",
    "                    group by p_id\n",
    "                    ) as c2\n",
    "                    on c1.p_id=c2.p_id and c1.Date_timestamp=c2.Datetime\n",
    "                    order by c1.p_id\"\"\")\n",
    "\n",
    "#converting dataframe into table\n",
    "table.registerTempTable('Table')\n",
    "\n",
    "\n",
    "#to update data\n",
    "table1=spark.sql(\"\"\" select *\n",
    "                   from\n",
    "                   (\n",
    "                    select *\n",
    "                    from Data_after_insertion \n",
    "                    where p_id not in(select p_id from Table)\n",
    "                    union\n",
    "                    select *\n",
    "                    from Table\n",
    "                    ) as ut\n",
    "                    order by ut.p_id\"\"\")\n",
    "\n",
    "\n",
    "#converting dataframe with deleted values into table\n",
    "table1.registerTempTable('Data_after_updation')\n",
    "\n",
    "\n",
    "#extracting historical data from the Data_after_insertion table which has been updated\n",
    "history_of_updated_products = spark.sql(\"\"\" select d.p_id,d.p_name,d.price,d.Date_timestamp\n",
    "                                            from Data_after_insertion as d\n",
    "                                            left join Table t\n",
    "                                            on d.p_id=t.p_id\n",
    "                                            order by d.p_id\"\"\")\n",
    "\n",
    "\n",
    "#printing the details\n",
    "print(\"Total updated=\",table.count())\n",
    "print(\"Total rows in main table after updating=\",table1.count())\n",
    "print(\"New updated id's=\",[int(row.p_id) for row in table.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:30.684583Z",
     "start_time": "2020-04-15T13:03:26.458313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total deleted= 8\n",
      "Total rows in main table after deleted= 103\n",
      "Deleted id's= [23, 31, 36, 47, 50, 71, 76, 97]\n"
     ]
    }
   ],
   "source": [
    "#for deleting the data\n",
    "\n",
    "#to retrieve the latest data from the data to be deleted\n",
    "table=spark.sql(\"\"\" select c1.p_id,c1.p_name,c1.price,c1.Date_timestamp\n",
    "                    from Changes_table as c1\n",
    "                    inner join\n",
    "                    (\n",
    "                    select p_id, max(Date_timestamp) as Datetime\n",
    "                    from Changes_table\n",
    "                    where record_type='D'\n",
    "                    group by p_id\n",
    "                    ) as c2\n",
    "                    on c1.p_id=c2.p_id and c1.Date_timestamp=c2.Datetime\n",
    "                    order by c1.p_id\"\"\")\n",
    "\n",
    "#converting dataframe into table\n",
    "table.registerTempTable('Table')\n",
    "\n",
    "\n",
    "#to delete data\n",
    "Final_latest_table=spark.sql(\"\"\" select * \n",
    "                    from Data_after_updation \n",
    "                    where p_id  not in(select p_id\n",
    "                                         from Table)\"\"\")\n",
    "\n",
    "\n",
    "#converting dataframe with deleted values into table\n",
    "Final_latest_table.registerTempTable('Data_after_deletion')\n",
    "\n",
    "\n",
    "#extracting data from the Data_after_insertion table which has been deleted\n",
    "deleted_data=spark.sql(\"\"\" select * \n",
    "                    from Data_after_insertion \n",
    "                    where p_id  in (select p_id\n",
    "                                    from Table)\n",
    "                    order by p_id\"\"\")\n",
    "\n",
    "#printing the details\n",
    "print(\"Total deleted=\",table.count())\n",
    "print(\"Total rows in main table after deleted=\",Final_latest_table.count())\n",
    "print(\"Deleted id's=\",[int(row.p_id) for row in table.collect()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:34.148836Z",
     "start_time": "2020-04-15T13:03:34.141746Z"
    }
   },
   "outputs": [],
   "source": [
    "#merging the historical data\n",
    "\n",
    "product_history = history_of_updated_products.union(deleted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:34.907575Z",
     "start_time": "2020-04-15T13:03:34.903249Z"
    }
   },
   "outputs": [],
   "source": [
    "#specifying path to export products history and new changes in the main table\n",
    "path_for_product_history='/home/bluepi/Downloads/result_2/product_history/'\n",
    "path_for_latest_product='/home/bluepi/Downloads/result_2/latest_product/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:38.503545Z",
     "start_time": "2020-04-15T13:03:35.549398Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporting the HISTORICAL data into csv files \n",
    "\n",
    "product_history.repartition(1).write.csv(path_for_product_history+yesterday_str, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:43.099572Z",
     "start_time": "2020-04-15T13:03:40.079318Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporting the FINAL TABLE\n",
    "\n",
    "Final_latest_table.repartition(1).write.csv(path_for_latest_product, header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T13:03:44.995961Z",
     "start_time": "2020-04-15T13:03:44.992716Z"
    }
   },
   "outputs": [],
   "source": [
    "#deleting the temp folder\n",
    "f.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
